m# Professional-Grade RAG System

A production-ready Retrieval-Augmented Generation (RAG) system with advanced features including hybrid search, confidence scoring, and conversational context management.

## ğŸŒŸ Features

### Core Capabilities
- **Hybrid Search**: Combines BM25 keyword search with semantic vector search
- **Intelligent Reranking**: Uses mxbai-rerank-large-v2 for optimal result ordering
- **Confidence Scoring**: Multi-factor confidence assessment (retrieval, coherence, coverage, clarity)
- **Conversational Context**: Maintains chat history for follow-up questions
- **Source Citations**: Every answer includes traceable source references
- **Professional UI**: Modern Next.js interface with real-time updates

### Technical Highlights
- **LlamaIndex Orchestration**: Modular, composable RAG pipeline
- **Groq Integration**: High-performance LLM inference (OpenAI gpt-oss 20b)
- **PostgreSQL + pgvector**: Robust vector storage and retrieval
- **Flexible Embeddings**: Support for local Ollama or remote embedding services
- **Type-Safe**: Full TypeScript support in frontend

## ğŸ“‹ Prerequisites

- **Python 3.11+**
- **Node.js 18+**
- **PostgreSQL** with `pgvector` extension
- **Groq API Key**
- **Ollama** 

## ğŸš€ Quick Start

### 1. Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your GROQ_API_KEY and DB settings

# Start backend
python -m app.main
```

### 2. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Configure environment
cp .env.example .env.local

# Start development server
npm run dev
```

### 3. Embedding Service (Optional)

If using the standalone embedding service:

```bash
cd embedding-service
ollama pull embeddinggemma
python main.py
```


### Query via UI

1. Open http://localhost:3000
2. Type your question in the input box
3. View answer with:
   - Confidence score (%)
   - Confidence level (high/medium/low)
   - Source citations with relevance scores

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js   â”‚  â† User Interface
â”‚  Frontend   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚  â† API Layer
â”‚   Backend   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LlamaIndex Pipeline    â”‚  â† RAG Orchestration
â”‚  - Hybrid Retriever     â”‚
â”‚  - BGE Reranker         â”‚
â”‚  - Confidence Scorer    â”‚
â”‚  - Context Manager      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL â”‚  Groq   â”‚   BM25   â”‚  â† Storage & Models
â”‚ (pgvector) â”‚  (LLM)  â”‚  (Index) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Project Structure

```
Professional_Grade_RAG/
â”œâ”€â”€ backend/                 # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ core/           # RAG engine, retriever, scorer
â”‚   â”‚   â”œâ”€â”€ models/         # Pydantic models, prompts
â”‚   â”‚   â”œâ”€â”€ services/       # Groq, Postgres, BM25
â”‚   â”‚   â””â”€â”€ utils/          # Logging, validation
â”‚   â”œâ”€â”€ data/               # Document storage
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/               # Next.js Frontend
â”‚   â”œâ”€â”€ app/               # Pages and layouts
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”œâ”€â”€ hooks/             # Custom React hooks
â”‚   â”œâ”€â”€ lib/               # API client, utilities
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ embedding-service/     # Standalone Embedding Service
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ docker-compose.yml
â””â”€â”€ README.md            # This file
```

## âš™ï¸ Configuration

### Backend Environment Variables

```bash
# Groq API
GROQ_API_KEY=your_api_key_here
GROQ_MODEL=openai/gpt-oss-20b

# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

# Embeddings
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=embeddinggemma
USE_REMOTE_EMBEDDING_SERVICE=false

# RAG Settings
CHUNK_SIZE=512
CHUNK_OVERLAP=128
TOP_K_RETRIEVAL=10
TOP_K_RERANK=5
```

### Frontend Environment Variables

```bash
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000
NEXT_PUBLIC_API_VERSION=v1
```

## ğŸ¯ Confidence Scoring

The system calculates confidence using four factors:

1. **Retrieval Score (55%)**: Quality of retrieved documents
2. **Answer Coherence (25%)**: LLM self-assessment
3. **Source Coverage (15%)**: Number and distribution of sources
4. **Query Clarity (5%)**: Quality of query understanding

**Confidence Levels:**
- ğŸŸ¢ **High (80-100%)**: Strong evidence from multiple sources
- ğŸŸ¡ **Medium (50-79%)**: Moderate evidence, some ambiguity
- ğŸ”´ **Low (<50%)**: Weak evidence, insufficient data

## ğŸ”§ Troubleshooting

### Database Connection
Ensure PostgreSQL is running and the `pgvector` extension is installed:
```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

### Groq API
Verify your API key is valid and set in the `.env` file.

## ğŸ“ˆ Performance Tips

1. **Chunk Size**: Adjust `CHUNK_SIZE` based on document type
2. **Top-K Values**: Tune `TOP_K_RETRIEVAL` and `TOP_K_RERANK`
3. **Confidence Weights**: Calibrate weights based on your use case

## ğŸ› ï¸ Development

### Adding New Features

**Backend:**
1. Add route in `backend/app/api/routes/`
2. Implement logic in `backend/app/core/` or `backend/app/services/`
3. Update schemas in `backend/app/models/schemas.py`

**Frontend:**
1. Create component in `frontend/components/`
2. Add to page in `frontend/app/page.tsx`
3. Update types in `frontend/lib/types.ts`


## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- **LlamaIndex**: RAG orchestration framework
- **Ollama**: Local LLM inference
- **ChromaDB**: Vector database
- **shadcn/ui**: UI component library
- **FastAPI**: Modern Python web framework
- **Next.js**: React framework

## ğŸ“ Support

For issues and questions:
1. Check the troubleshooting section
2. Review logs: `docker-compose logs`
3. Consult `PLAN.md` for architecture details

---

**Built with â¤ï¸ for professional RAG applications**
